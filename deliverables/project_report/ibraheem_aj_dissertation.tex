\documentclass[a4paper,12pt]{report}

\usepackage[]{geometry} %textwidth=6.5in,textheight=9in
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{url}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{minted}

\hypersetup{ colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan }

\begin{document}

\begin{titlepage}
	\begin{center}
		%\vspace*{1cm}
		\includegraphics[scale=1,width=0.3\textwidth]{images/uob_logo} \\
		\huge
		\textbf{Machine Learning in Drug Discovery and Design\\}
		\vspace{0.5cm}
		\Large
		Predicting the Blood Brain Barrier Penetration of Drugs \\
		\vspace{1.5cm}
		\textbf{Ibraheem Ajibola Ganiyu}
		\vfill
		A dissertation presented for the degree of \\
		BSc. (Hons) Software Engineering. \\
		\vspace{0.8cm}
		\large
		School of Computing, Engineering and Mathematics \\
		University of Brighton \\
		\date{\today}
	\end{center}
\end{titlepage}

\pagenumbering{roman}
\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
\pagenumbering{arabic}


\begin{abstract}
	Drug design and discovery is a very expensive process and lots of new compounds are being developed rapidly. Only roughly about 2\% of drugs can pass through the blood brain barrier, this presents a problem in Central Nervous System (CNS) drug development. \\
	This Project aims to develop a solution that can predict with high confidence, the probability of a drug passing through this blood brain barrier in hopes that this can speed up the process of developing a CNS drug.
\end{abstract}

\chapter{Introduction}
	Chemical data is growing exponentially as there are currently more than 123 million organic and inorganic substances to date \cite{CAS2016}, which means for drug development purposes, there is an abundance of chemical data to analyse in search for ideal candidates for development. \\
	Machine Learning is a form of artificial intelligence (AI) that enables computer programs to learn concepts from data without being explicitly programmed. This technique of AI can be applied to problems in many domains, which is what this paper aims to do by applying it to chemical data to build computer models, aka Virtual Screening, models which can predict with high accuracy, the probability of a drug to pass through the blood brain barrier (BBB) of living organisms. \\
	\textit{In Silico} models (computer models) have a profound use in virtual screening as they can enable scientist to scan through a large database of drugs to speed up a time consuming process of analysing drug candidates. \\
	In the context of CNS (Central Nervous System) drug development, when a drug is absorbed into the blood stream, it needs to be able to pass through the Blood Brain Barrier (BBB) to its target which could be the brain or the nervous system, an example would an anti-migraine agent, ergotamine, 
		\begin{figure}[H]
			\centering
			\includegraphics[scale=1,width=0.55\textwidth,totalheight=0.3\textheight]{images/Ergotamine-skeletal} 
			\caption{Chemical Representation of Ergotamine}
			\label{fig:ergotamine}
		\end{figure}
	which has to pass through the blood brain barrier to the lining of the brain where it constricts the blood vessels there to decrease the pain from migraine headaches \cite{DrugsCom}. 
	%TODO: This needs citation -> 2% of known molecules
	However statistically speaking, only 2\% of the currently known molecules can pass through this blood brain barrier, which translates to more time in the drug discovery pipeline spent on analysing drug candidates that can pass through the BBB barrier. \\
	\section{Proposed Solution}
		This problem of determining the drug candidates that belong to this 2\% is a very challenging one and this paper approaches the problem through the use of computer models built with machine learning techniques, applied to a large database of molecules that pass through the Blood Brain Barrier, with the aim of predicting the probability of an unknown candidate drug passing through the BBB. \\
		Ana et al \cite{Anaetal2012} points out that there is a lack of extensive dataset on BBB prediction as most of them are not comprehensive enough to build complex models out of, as a result, they have compiled a dataset of 2040 molecules for use in BBB prediction. Based on analysis carried out \cite{Anaetal2012} by Ana et al, They show that certain machine learning classifiers such as support vector machines and random forests outperform other classifiers especially for BBB classification tasks. This paper will attempt utilise that analysis as a baseline for developing our computer models whilst also exploring the possibility of Deep neural networks, as Thomas et al \cite{Thomasetal2014} show that Deep learning techniques have a significant opportunity in virtual screening.
	\section{Results}


\chapter{Background}
	Everything around us is composed of molecules, they are an electrically neutral group of two or more atoms held together by chemical bonds. They are the smallest particle in a compound that exhibit the chemical properties of the compound. 
	Trees can be said to be made up of molecules and historically parts of trees have always been used to cure or alleviate symptoms of illness. Over time, the individual molecules in these herbal medicines were recognized for their effects and they were being produced synthetically, which further gave rise to Modern Drug Design and Discovery. \\
	A potential molecule aka drug candidate is usually screened against a target protein to test its effectiveness and the screening can either be virtual (Virtual Screening) or through a method known as High Throughput Screening (HTS) - a method of experimentation involving the use of robots and control software to conduct millions of scientific tests. These HTS machines can also be credited with the exponential growth in chemical data \cite{Dougetal2008}.
	Drug discovery is a very expensive and time consuming process; It is usually broken down into numerous stages with the most expensive stage being the clinical trials. \\
	The problem of the Blood Brain Barrier (BBB) prediction evolves around Chemistry and Computer Science being applied to the Drug Design domain with a thorough understanding of the constrainsts imposed by the Central Nervous System. This chapter introduces the necessary concepts needed to understand the solution taken to the BBB prediction problem.
	
	
	\section{Cheminformatics}
	Cheminformatics aka Chemoinformatics and Chemical Informatics can be visualised as a cross domain of Chemistry and Computer Science. As defined by Brown, it is the mixing of numerous information that a scientist needs to transform data into information for the intended purpose of making better decisions in drug lead identification and optimisation \cite{FKBrown1998}. \\
	The applications of Cheminformatics that are of particular interest are \textit{Storage and Retrieval of Chemical Data} and \textit{Virtual Libraries and Screening} as they both would enable the manipulation and transformation of molecular information for our machine learning algorithms. 
		\subsection{Representing Molecules in Computers}
		The two most common formats for representing chemical molecules in computers are Simplified Molecular-Input Line Entry System (SMILES) and SMART, with SMART being created by Daylight Chemical Information Systems and both formats being actively supported by them \cite{OpenBabel2017}.
			\paragraph{SMILES} are specifications in the typographical notation system that describe the structure of a molecule using short ASCII strings and they are more commonly used. Water can be written in the SMILE format as $[OH2]$ An example would be the SMILE representation of protriptyline
				\begin{equation*}
					CNCCCC1c2ccccc2C=Cc3ccccc13
				\end{equation*}
			This format can then be utilised by cheminformatics software to extract meaningful chemical information about the molecule. Throughout the project, the Open-Source Cheminformatics Software, \href{www.rdkit.org}{RDKit}, was used to transform the SMILEs in the dataset and also for the extraction of molecular information, which will be explained below.
		\subsection{Molecular Representation and Similarity }
		The simplest way to cluster molecules together in a chemical database when performing virtual screening is through the concept of \textit{molecular similarity}. It is the core of Molecular Similarity Analysis (MSA) where the similarity measure, that characterizes the degree of proximity between pairs of molecules are manifested by their "molecular patterns", which are compromised of sets of features (chemical descriptors) \cite{Jurgen2004}. \\
		Many different molecular similarity measures exists and some are more suitable to certain virtual screening tasks than another, which is what prompts the discussion of molecular similarity measures in section \ref{subsubsection_similarity_measures}. The motivation behind the discussion is that it is possible to compare the efficiency of each similarity measure in terms of computer resources (computer memory and time) so as to determine its applicability to the target chemical dataset.\\
		To determine the similarity between molecules, some form of similarity measure has to exist to compare molecules in the same representation. The similarity measures (similarity coefficient) are functions that maps pairs of compatible molecular representation into a real number. 
				\subsubsection{Molecular Representation} 
				%Using the Open-Source Cheminformatics Software, RDKit, it is possible to transform 
				There also is the notion of a chemical space that maps the chemical features into some form of structure, which are mostly coordinate-based. \\
				Some common representations of molecules include encoding the molecular data into a set, graph, vector or function-based representation that uses distance as a form of molecular similarity \cite{Jurgen2004}. Each representation has its advantages and disadvantages, where graphs have their shortcomings, the molecule could be represented as a feature vector of its chemical properties which can be its molecular fragments, partial atomic charge, molecular weight, logP etc. \\
				For our blood brain barrier prediction task, the molecules in the training set were transformed into their molecular fingerprint representation and simple molecular descriptors feature set. Here the molecule is represented as a point in a 2D or 3D coordinate space based on the derived features by performing Principal Component Analysis (PCA) or Non-linear Mapping (NLM) on the original set of features of the molecule.  
				\paragraph{Molecular Fingerprints as a Binary-Valued Feature Vector}
				Each fingerprint consists of an n-component bit vector, 2048 bits for a Morgan fingerprint representation but the sizes of the vectors can vary depending on the kernel function used. The vector is given by 
					\begin{equation}
					\vec{V_A} = (v_A(x_1), v_A(x_2),...,v_A(x_k),...,v_A(x_n))
					\end{equation}
				where $x_k$ indicates the absence or presence of a given feature \cite{Jurgen2004}. i.e
					\begin{equation}
					v_A(x_k) = 
					\begin{cases}
					1 & \text{Feature present} \\
					0 & \text{Feature absent}
					\end{cases}
					\end{equation}
			\subsubsection{Similarity Measures} 
			\label{subsubsection_similarity_measures}
			For our prediction task, the main use of molecular similarity measures is in similarity searching when performing the nearest neighbour search. Here the molecules are ordered by their chosen chemical descriptors when we apply our similarity measure to calculate some form of structural relatedness between a target molecule and every other molecule in the dataset. The result is then a sorted list of molecules where the most similar molecules to our target molecule are located at the top of the list and in order of decreasing similarity.\\
			According to Jurgen \cite{Jurgen2004}, the most important components of similarity measures are
				\begin{itemize}
					\item \underline{The Representation}: Which is used to characterize the molecules that are being compared. An example would be molecular fingerprint representation of a molecule.
					\item \underline{The Weighing Scheme}: Used to assign differing degrees of importance to the various components of the molecular representation. An extra measure of accuracy in the weighing schemes would be the use of a chemical ontology database (e.g CheBL) when determining the importance of each component \cite{AnaPhd2014}.
					\item \underline{Similarity Coefficient}: A quantitative measure of the degree of structural relatedness between two molecules.
				\end{itemize}
			The main application of any similarity measure is to the fact that \textbf{structurally similar molecules exhibit similar properties} as stated by Johnson and Maggiora (1990) \cite{JohnMaggiora1990}. However, they also noted that an exception to the rule is that sometimes a small change in the structure of the molecule can result in a radical change in the properties that it exhibits. Which is why Ana Teixeira (2014) \cite{AnaPhd2014} notes that the use of a chemical ontology database would be a plausible method to combat this exception. For our prediction task, the exception to this rule is ignored for practical purposes as majority of molecules exhibit similar properties to one another.\\
			In similarity measure calculations, the most common measure for calculating fingerprint similarity is the Tanimoto coefficient, given by 
				\begin{equation}
				Tanimoto(\vec{v_i}, \vec{v_j}) = 
				\frac{\vec{v_i}\bullet\vec{v_j}}
				{\sum_{k}v_{ik} + \sum_{k}v_{jk} - \vec{v_i}\bullet\vec{v_j}  } 
				\end{equation}
			where $\vec{v_i}$ and $\vec{v_j}$ are the bit vectors for molecule $i$ and $j$. There are other similarity coefficients such as the Tversky and Dice, the Dice similarity measure was chose for the k-nearest neighbour calculation of dataset.
	
	\section{Drug Design and Discovery}
	According to Dr A.N Boa \cite{hull2016}, the different stages of drug discovery are
		\begin{itemize}
			\item Programme Target Selection (Choosing the disease to work on)
			\item Identification and Validation of the drug target
			\item Assay Development
			\item Identification of a Lead Compound
			\item Lead Optimisation
			\item Identification of a drug candidate 
			\item Clinical Trials 
			\item Release of the drug 
			\item Follow-up Monitoring
		\end{itemize}
	Majority of the targets for the drugs we consume are usually proteins e.g enzymes, receptors and nucleic acids and the structure of the target is confirmed through a virtual screening method known as \textit{molecular docking}; it can be used to predict how the drug will bind to its target protein though various search/optimisation algorithms \cite{Jurgen2004}.
	Another Virtual Screening technique usually used is \textit{Quantitative Structure-Activity Relationships} (QSAR), here the underlying idea is that molecules with similar structures behave in the same way, as a result, the activity of a protein against a certain group of compounds is recorded and a QSAR model is the constructed from there and used to determine whether a given compound will bind to the target, thus screening the virtual compound library for drugs of interest.
	
	\section[CNS Drug Design and the Blood Brain Barrier]{Central Nervous System Drug Design and the Blood Brain Barrier}
	CNS Drugs aiming to pass through the Blood Brain Barrier often need to possess certain physical-chemical properties; some of which are Hydrogen bonding, ionization properties, molecular flexibility etc. \cite{Hassanetal2005}. The epithelial cells form an interface between the blood and the brain and these are commonly referred to as the Blood Brain Barrier. This interface occurs in other places within the body but what makes the BBB epithelial cells different are the tight junctions they form which makes it harder for drugs to pass through. Hassan and George (2005) further claim in their article that the majority of BBB penetration is through passive diffusion through the cellular membrane.
		\subsubsection{ADME Properties of CNS Drugs}
		For a CNS drug to be therapeutically effective, it must be easily disposed asides from having a high degree of potency. The ADME properties of a drug refers to its ability to be easily absorbed, distributed, metabolised and excreted. Some properties of CNS drugs affect their ADME properties, some of which are \cite{Hassanetal2005}:
			\begin{itemize}
				\item \underline{Solubility}: A drug must be very soluble in the blood and still be in high enough concentration at its target, in this case, the Blood Brain Barrier, so that it can easily be absorbed.
				\item \underline{Amount of Protein Binding}: Majority of CNS drugs tend to have high binding property towards proteins - this results in the drug being metabolised easily.
				\item \underline{Partition Coefficient (LogP)}: This is sometimes referred to as the \textit{lipophilicity} of the compound and has served as one of the most important factors in drug design. Higher lipophilicity results in drugs with higher metabolic turnover but lower solubility and absorption \cite{Hassanetal2005}. 
			\end{itemize}

	\section{Machine Learning concepts}
	% Here the basic concepts about them would be discussed and an overview provided, this section should not pertain to the drug discovery project but machine learning in general
		\subsection{Models and Classifiers}
		\subsection{Feature Extraction}
	
	

\chapter{Data Representation and Feature Engineering}
% Just like the machine learning concepts section in the Background chapter, the concepts would be described in great detail in the context of drug discovery and the blood brain barrier.
	\section{Data Preprocessing}
		% Include notes on the technologies used e.g Numpy and Pandas
	\section{Feature Extraction}
	
\chapter{Machine Learning Classifier Training}
	\section{Neural Networks}
	\section{Ensemble Classifiers}

\chapter{Model Evaluation and Improvement}
	\section{Model Evaluation}
		% Draw a diagram like the one in this link(http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/) to compare the results of all the classifiers
	\section{Model Improvement techniques}
		\subsection{Grid Search}
	\section{Model Persistence} 
	%How you will use pickle to persist the model
		
\chapter{Conclusion}
	\section{Integration into a Web Application}
	\section{Deployment}
	\section{Areas for improvement}


\chapter[Notes]{Notes: Machine Learning in Drug Design (Will be removed later)}
	With the recent explosion in chemical data available, it is possible to combine them with machine learning techniques to create models of molecules that have a strong binding affinity to their target proteins. According to Antonio (2015) \cite{Antonio2015}, Virtual screening techniques can either be Structure-Based or Ligand-Based. Structural screening used the idea of molecular docking as described earlier whilst Ligand-Based screening uses the idea that similar molecules exhibit similar properties e.g QSAR. 
	\\
	With Ligand-Based Virtual Screening being the core focus of this paper, we can further classify them into either similarity searching or compound classification \cite{Antonio2015}. Antonio (2015) \cite{Antonio2015} further highlights that the following are the most popular and successful techniques for Ligand-Based Virtual Screening:
	\begin{itemize}
		\item Support Vector Machines
		\item Decision Trees
		\item Random Forests (An Ensemble of Decision Trees)
		\item Naive Bayesian Classifiers
		\item k-Nearest neighbours
		\item Artificial Neural Networks
	\end{itemize}
	\section{Data Preprocessing and Feature Engineering}
		\subsection{Data Visualisations}
		Attempting to explore how the data looks, we start with a scatter plot of the data. Due to the high dimensionality of the data. It is near impossible to plot a scatter plot in 2D, so we apply a Principal Component Analysis (PCA) on the data to extract the two most important component as shown below
			\paragraph{Manifold Learning with t-SNE}
			Manifold Learning Algorithms excel at data visualisation tasks, they provide more complex mappings and often better data representations but perform poorly for data classification tasks. They perform poorly because they do not allow transformations of new data once they have been fitted.\\
			The t-SNE algorithm is used because it tries as much as possible to maintain the distance between the data points in the original feature vector space during transformation. It puts more emphasis on points that are close by rather than preserving points that are far apart \cite{Mueller2016}.
				\begin{figure}[H]
					\centering
					\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/scatter_tsne}
					\caption{2D Scatter Plot after applying the t-SNE Algorithm}
					\label{fig:scatter_tsne_2D}
				\end{figure}
			Looking at the t-SNE scatter plot, there is a small cluster of the molecules that do not pass through (B) at the top and some randomly distributed throughout the plot, whilst the ones that pass through the BBB barrier (A) maintain an even distribution around the feature space, it is worth mentioning that performing passing the simple molecular data to the algorithm, it was preprocessed with a MaxAbsoluteScaler that scales each feature by its maximum value. This doesn't mean that it is impossible to separate the data, as an accuracy of $\sim89\%$ will be achieved in later sections. This result is to be expected as the data is biased towards molecules that cross the barrier and it also hints at the fact that the molecules that do not cross the barrier might share a lot in common with molecules that cross the barrier.
				\begin{figure}[H]
					\centering
					\includegraphics[width=\textwidth,scale=1,totalheight=0.5\textheight]{images/scatter_tsne_mfps}
					\caption{2D Scatter Plot after applying the t-SNE Algorithm (Morgan Fingerprint dataset)}
					\label{fig:scatter_tsne_mfps_2D}
				\end{figure}
			The scatter plot has no significant difference with the Morgan Fingerprint dataset, it highlights the clusters of the B molecules more visibly. Further analysis would need to be carried to create a more linear separable representation.
				\begin{figure}[H]
					\centering
					\includegraphics[width=\textwidth,scale=1,totalheight=0.5\textheight]{images/scatter_tsne_3D}
					\caption{3D Scatter Plot after applying the t-SNE Algorithm}
					\label{fig:scatter_tsne_3D}
				\end{figure}
			The 3D scatter plot of the simple molecular descriptor data looks slightly more separable than its 2D counterpart.
			\paragraph{Principal Component Analysis (PCA)} 
			In the 2D representation shown below, the data points are tightly clustered in the middle, and looks like it is linearly inseparable.
				\begin{figure}[H]
					\centering
					\includegraphics[width=\textwidth,scale=1,totalheight=0.5\textheight]{images/scatter_pca}
					\caption{2D Scatter Plot after applying the PCA Algorithm}
					\label{fig:scatter_pca_2D}
				\end{figure}
			Remembering that the dataset is biased towards the molecules that pass through the barrier, as they represent around 70\% of the data. A significant proportion of the molecules that do not pass through the barrier(B) are clustered in the same space as the ones that pass through the barrier (A). Although majority of A stays clustered in the same space with the exception of a few outliers.\\
			To further examine the data, we look at creating a 3D representation, the next 3 principal components are selected to create data for the 3D plot. The 3D plot has shown below also has the same clustering of a significant proportion of the B molecules in the same space as the A molecules but looks more linearly separable than the 2D plot
				\begin{figure}[H]
					\centering
					\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/scatter_pca_3D}
					\caption{3D Scatter Plot after applying the PCA Algorithm (I)}
					\label{fig:scatter_pca_3D}
				\end{figure}
				\begin{figure}[H]
					\centering
					\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/scatter_pca_3D_2}
					\caption{3D Scatter Plot after applying the PCA Algorithm (II)}
					\label{fig:scatter_pca_3D_2}
				\end{figure}
			\subsubsection{Clustering}
				\paragraph{k-Means Clustering}
				In our further attempts to have some form of linearly separable data, we select the 2 principal components of our datasets and apply the k-means clustering algorithm on them.
					\begin{figure}[H]
						\centering
						\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/kmeans_smd_scatter}
						\caption{2D Scatter Plot found by k-means on Simple Molecular Descriptors. \textit{Original data points on the right and clusters found on the left}}
						\label{fig:kmeans_smd_scatter}
					\end{figure}
				We can see that the clustering algorithm performs poorly on the simple molecular descriptor dataset. Examining the data, it clustered 2039 molecules to class 0 and clustered just 1 molecule to class 1. \\
				However, promising results were achieved using the Morgan fingerprint dataset.
					\begin{figure}[H]
						\centering
						\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/kmeans_morgan_scatter}
						\caption{2D Scatter Plot found by k-means on fingerprint dataset. \textit{Original data points on the right and clusters found on the left}}
						\label{fig:kmeans_morgan_scatter}
					\end{figure}
				The plot looks linearly separable now, an accuracy of 84\% was achieved classify the red points (molecules that pass through the barrier (A)) and a 59\% accuracy of classifying the blue points.The yellow triangles in the diagram represent the centers of each clusters.
				\paragraph{Agglomerative Clustering}
				For the Simple Molecular dataset, the entire dataset was scaled to have a minimum of -1 and a maximum of 1. Then the 2 principal components were selected, a ward agglomerative cluster algorithm was applied and it created 2 clusters with cluster 1 having 1442 samples and cluster 2 having 598 samples. The original dataset had 1563 molecules passing through the barrier and 477 molecules not passing through, most likely cluster 1 represents the molecules that pass through
					\begin{figure}[H]
						\centering
						\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/agg_smd_scatter}
						\caption{2D Scatter Plot of clusters found by the agglomerative ward algorithm on the simple molecular dataset}
						\label{fig:agg_smd_scatter}
					\end{figure}
				Applying the same cluster to the Morgan fingerprint dataset, without preprocessing the data, it created 2 clusters; cluster 1 with 1894 samples and cluster 2 with 146 samples. The simple molecular descriptor dataset proves to be a better dataset for this clustering algorithm as it achieves a lower error rate
					\begin{figure}[H]
						\centering
						\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/agg_morgan_scatter}
						\caption{2D Scatter Plot of clusters found by the agglomerative ward algorithm on the Morgan Fingerprint dataset}
						\label{fig:agg_morgan_scatter}
					\end{figure}
				
				
	
	\section{Machine Learning Classifiers/Models}
		A Detailed explanation of the selected classifiers based on literature review
		\subsection{Decision Tree}
		Decision trees are an example of non-parametric supervised learning algorithms.
		After applying the decision tree classifier, an accuracy of $\sim85\%$ was achieved. Examining the internal workings of the Tree, based on the simple molecular descriptor dataset, we can see that the total polar surface area (TPSA) is the defining factor in determining whether a molecule crosses the blood-brain barrier.
			\begin{figure}[H]
				\centering
				\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/bbb_tree_smd}
				\caption{Decision Tree: Simple Molecular Descriptors}
				\label{fig:bbb_tree_smd}
			\end{figure}
			\subsubsection{Random Forests}
			An accuracy of $\sim87\%$ was achieved using the random forest classifier on the simple molecular descriptor dataset whilst an accuracy of $\sim89\%$ was achieved on the Morgan fingerprint dataset. The number of trees in the random forest was set to 20.
			\subsubsection{Extra Trees}
			The extra trees tend to perform slightly better than the random forests. On the simple molecular dataset, the extra trees score an accuracy of $\sim90\%$ and on the Morgan fingerprint dataset, the extra trees score an accuracy of $\sim89\%$, performing just slightly less than the random forest on the same dataset. The number of extra trees was set to 20
			\subsubsection{AdaBoost Decision Trees}
			Applying the AdaBoost algorithm to the decision trees, an accuracy of $\sim83\%$ was achieved on the simple molecular descriptor dataset, and an accuracy of $\sim81\%$ on the Morgan fingerprint dataset. Both rounds of classification with number of trees = 200. Strangely, the Adaboosted decision trees ought to provide better results than the random forest or extra trees classifiers. 
			\subsubsection{Gradient Tree Boosting}
			With a little bit of tuning the parameters of the gradient boosted classifier, by increasing the number of estimators to around 150 and reducing the learning rate to 0.45. An accuracy score of $\sim86\%$ was achieved on the simple molecular descriptors and $\sim82\%$ on the Morgan fingerprint dataset.
			
		\subsection{K-Nearest Neighbours (kNN)}
		The kNN classifier, at $k=10$ has an accuracy of roughly around $\sim85\%$ using only the simple molecular descriptors. By engineering the feature vector to have unit variance and a Gaussian mean of 0, we then have an accuracy of around $\sim89\%$ at 10 neighbours. 
			\begin{figure}[H]
				\centering
				\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/knn_molecular_descriptors}
				\caption{kNN classifier for 10 neighbours using the Simple Molecular Descriptors}
				\label{fig:knn_smd}
			\end{figure}
		Also using the Morgan fingerprint has the data for the kNN classifier, we have an accuracy of around $\sim89\%$. Without scaling the simple molecular descriptors, the Morgan fingerprint dataset provides better results.
			\begin{figure}[H]
				\centering
				\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/knn_morgan_dice_fingerprint}
				\caption{kNN classifier for 10 neighbours using the Morgan Fingerprint dataset}
				\label{fig:knn_mfps}
			\end{figure}
		
		\subsection{Support Vector Machines}
		Scaling the data using a Standard Scaler and selecting the top 100 principal component, a mean accuracy of 84.9\% was achieved using both datasets
			\begin{figure}[H]
				\centering
				\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/mol_decision_svm}
				\caption{Support Vector Machines with different kernel functions on Simple Molecular Dataset}
				\label{fig:svm_mol}
			\end{figure}
			\begin{figure}[H]
				\centering
				\includegraphics[width=\textwidth,scale=1,totalheight=0.4\textheight]{images/fps_decision_svm}
				\caption{Support Vector Machines with different kernel functions on Morgan Fingerprint Dataset}
				\label{fig:svm_fps}
			\end{figure}
		\subsection{Neural Networks}
		The Human brain is a very fascinating and robust system, the basic processing unit of the brain is the nerve cell or neuron and there are around 100 billion neurons in the brain all connected together by synapses. These neurons are electrically excitable, they pass signals from one neuron to the next by electrical and chemical signals when the membrane potential reaches some threshold, which can be inhibited if the threshold is not exceeded. A Neural Network is a computational model inspired by the human brain, it consists of a network of neurons connected together by axons. The inspiration behind the model is that if learning occurs in the brain and if we can represent this in a computational model then we can achieve some form of intelligence in our systems.
			\paragraph{Hebbian Learning}
			This is a learning concept of the brain based on the mechanism neural plasticity proposed by Donald Hebb. The rule formulated states that changes in the strength of synaptic connections are proportional to the correlation in the firing of the two connecting neurons, the strength of the connection is proportional to the frequency of excitement between them. However, the connection between two neurons that never fire will eventually be broken. \\
			The idea of repetition enhances retention can also be applied here, where actions that are repeated continuously will create retention in the brain \cite{Cunninghametal1984}. An example is in the experiment carried out by Ivan Pavlov, where his dogs were conditioned to associate the ringing of a bell to the presence of food. As a result, the neurons responsible for salivating over food and for hearing the bell would have a very strong connection to the extent that merely hearing the bell was sufficient to cause the salivating neurons to fire. 
			\subsubsection{Multi Layer Perceptron}
			The Perceptron can also be referred to a collection of neurons along with a vector of inputs and a vector of weights to fasten the inputs to the neuron. This neuron in this network consists of its inputs, weights, threshold and activation function. Each neuron,receives an input along with the strength of the input i.e weight, which it multiplies together and if the value is greater than the threshold, the neuron fires. \\
			For our dataset the input $X = [x_1,x_2,x_3,...,x_n]$ to a Perceptron could be either a simple molecular feature vector value e.g the number of hydrogen atoms in the molecule or vectors of 1s and 0s if we're using the Morgan fingerprint dataset to train the network. Each neuron is also given a vector of small random positive and negative values as its weights. The neuron calculates its activation value as 
			\begin{equation}
				h = \sum_{i=1}^{m}w_ix_i
			\end{equation}
			This activation value is then passed on to the activation function $f(h)$ which fires if the activation value is greater than the threshold. A threshold value of 0 was used irrespective of the training dataset. The output value of the function is given by
			\begin{equation}
				output = f(h) = 
				\begin{cases}
					1 & \text{if h} \textgreater \theta \\
					0 & \text{if h} \le \theta
				\end{cases}
			\end{equation}
			\paragraph{Multi Layer Perceptron}
			The Input layer to the neuron is the same as the length of the feature vector plus one (the fixed bias input). For the simple molecular feature dataset, the number of inputs $n$ to the network is 196, as that is the number of simple chemical descriptors that were calculated. For the Morgan Fingerprint dataset, $n = 2048$ where $n$ represents the length of the bit vector. \\
			The bias input is used as a control value to adjust the value that the neuron fires at. An example is when the feature vector to a neuron is all zeros and the neuron should fire. By our activation function, the neuron wouldn't fire because its activation value does not exceed the threshold. By adding a fixed value (bias) of $\pm1$ to the feature vector, we can then adjust the weight to make the neuron fire or not.
			\begin{figure}[H]
				\centering
				\includegraphics[width=\textwidth,scale=1,totalheight=0.5\textheight]{images/multilayerperceptronnetwork}
				\caption{Multi Layer Perceptron \cite{ScikitLearn2016}}
				\label{fig:mlp}
			\end{figure}
			\subsubsection{Application to the Blood Brain Barrier prediction}
			We are undertaking a form of supervised learning in the BBB prediction. The dataset we have compiled is labelled with the correct class and this data is fed to the neural network in order for the it to generalise what it has learned about our data. The network will then find patterns in the molecules that cross the blood-brain barrier and predict if an unknown molecule will cross it or not. \\
			Let $X = [x_1,x_2,x_3,...,x_n]$ be the input vector to our network for $n$ features and $W=[w_1,w_2,w_3,...,w_n]$ , the weights. Each neuron calculates whether it should fire or not based on its activation vector. We then examine the wrong neurons i.e the neurons that fired when they should have not. For each wrong neuron $i$, we calculate the difference in weights for it $\Delta w_i = -(y_i - t_i) * x_i$ where $y_i$ is the output and $t_i$ is the target output, the weight for neuron $i$ is then updated as $w_i = w_i + \Delta w_i\eta$, where $\eta$ is the learning rate (usually around $0.1 < \eta < 0.4$) which determines how much to change the weights by and as a result, determines how fast the network learns. The training is run again till the algorithm converges and the network gets all of its input right.
			\paragraph{The Learning Algorithm \cite{StephenM2014}}
			\begin{itemize}
				\item \textbf{Initialisation}
					\\ $n$ small (positive and negative) random numbers are assigned as weights $w_{ij}$ where $i$ is the number of weights and $j$ represents the neuron being examined.
				\item \textbf{Training} for T iterations or until all the outputs are correct
					\begin{itemize}
						\item[-] For each input vector $X = [x_1,x_2,x_3,...,x_n]$
							\begin{itemize}
								\item[*] Compute the activation value ($h$) of each neuron $j$: \\
									\begin{equation}
										h = \sum_{i=0}^{n}w_{ij}x_i
									\end{equation}
								\item[*] Calculate the activation ($y$) of neuron $j$ using the activation function $g$: \\
									\begin{equation}
										y_j = g(h) = 
										\begin{cases}
											1 & \text{if h} \textgreater \theta \\
											0 & \text{if h} \le \theta
										\end{cases}
									\end{equation}
								\item[*] Update the weights of neuron $j$ using the formula: \\
									\begin{eqnarray}
										\Delta w_{ij} = - \eta(y_j - t_j) * x_i & \\
										w_{ij} = \Delta w_ij + w_ij
									\end{eqnarray}
								
							\end{itemize}
					\end{itemize}
				\item \textbf{Recall or Prediction}	
					\\To predict the value of new molecule with feature vector $X$ using:
					\begin{equation}
						y_j = g(\sum_{i=0}^{m}w_{ij}x_i) = 
						\begin{cases}
							1 & \text{if } w_{ij}x_i \textgreater 0 \\
							0 & \text{if } w_{ij}x_i \le 0
						\end{cases}
					\end{equation}
					where 1 represents a molecule that passes through the barrier and 0 a molecule that doesn't.
			\end{itemize}
		\paragraph{Model Training}
		For both datasets (simple molecular descriptors and fingerprint), they were rescaled to have a minimum of -1 and a Maximum of 1. The transformed data was then trained on a neural network and an accuracy of $\sim86\%$ was achieved. \\
		\subparagraph{Model Selection on the datasets for training} 
		The following model selection techniques were applied to both datasets
			\begin{itemize}
				\item \underline{Univariate Model Selection}: Here for each feature, we compute whether there is a statistical relationship between the feature and the target. The features which are calculated to be highly unrelated to the target are then dropped from the dataset. Applying this model selection to the molecular descriptor dataset and using a percentile of 50 gives us the features in figure \ref{fig:neuralnetworksmdunivariate}.
					\begin{figure}[h]
						\centering
						\includegraphics[width=\textwidth,scale=1]{images/neural_network_smd_univariate_matrix}
						\caption{Matrix diagram of selected features}
						\label{fig:neuralnetworksmdunivariate}
					\end{figure}
				The neural network was then trained with the new feature vector and a percentage increase in accuracy was achieved - with a new accuracy of $\sim87\%$ on the molecular descriptor dataset and an accuracy of $\sim88\%$ on the fingerprint dataset.
			\end{itemize}
			\subsection{Classifier Performance Comparison}
			\begin{table}[ht]
				\caption{Classifier average performance on different datasets}
				\centering
				\begin{tabular}{c|c|c}	
					\hline\hline
					Classifier & Simple Molecular descriptors(\%) & Morgan Fingerprint(\%) \\
					\hline
					Decision Tree & 80 & 79 \\
					Extra Trees & 86 & 84 \\
					Random Forests & 85 & 84 \\
					AdaBoost Decision Trees & 84 & 82 \\
					Gradient Boosted Trees & 86 & 82 \\
					k-Nearest Neighbours & 87 & 89 \\ 
					Support Vector Machines & 84.9 & 84.9 \\
					Neural Networks & 87 & 88 \\
					\hline
				\end{tabular}
			\end{table}
			%\subsubsection{Deep Learning}
		%\subsection{NaÃ¯ve Bayesian Classifier}




	\section{Model Evaluation and Improvement}
	%TODO: Talk about Cross validation, include diagrams like in scikit-learn pg 252, the entire chapter is a valuable resource	

%\chapter{Requirements}


%\chapter{Design}


%\chapter{Implementation}


%\chapter{Testing and Evaluation}


%\chapter{Results}

\bibliographystyle{plain}
\bibliography{ibraheem_aj_dissertation}

\end{document}